{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled17.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOSAGltDBbhxJp/MdMkRTz6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shouvikcirca/WaterPrediction/blob/main/RNN_Succesful.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IGqeaCBr8rlU"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "! unzip gdrive/MyDrive/Water_Prediction/WaterQuality_1.zip\n",
        "! unzip \"Water Quality\"/DataSet/WaterQualityData.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "eD9lKmAi9Az0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def getData():\n",
        "  devicemetadata = pd.read_csv('WaterQualityData/devicemetadata-tng.csv')\n",
        "  flowmeterdata = pd.read_csv('WaterQualityData/flowmeterdata-tng.csv')\n",
        "  sensordata = pd.read_csv('WaterQualityData/sensordata-tng.csv')\n",
        "  return devicemetadata, flowmeterdata, sensordata"
      ],
      "metadata": {
        "id": "6xjAgEPS8t2k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "devicemetadata, flowmeterdata, sensordata = getData()"
      ],
      "metadata": {
        "id": "_18kFwj18t5O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Doing a Full Outer Join on devicemetadata and flowmeterdata on the 'deviceid' column\n",
        "devicemetadata_flowmeterdata_merged = pd.merge(devicemetadata, flowmeterdata,  on='deviceid', how='outer')"
      ],
      "metadata": {
        "id": "I3QCUfaJ8t71"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "newFrame = devicemetadata_flowmeterdata_merged.copy()"
      ],
      "metadata": {
        "id": "8bTckp6e8t-j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "newFrame.duplicated().sum() # No duplicates"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lDrtIWH73sKb",
        "outputId": "f8d659fe-94cf-47af-b514-568cf3824fc9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "datetimeFrame = newFrame.copy()[newFrame['villagename'] == 'Agling'].sort_values('datetime')[['datetime','quantity']]"
      ],
      "metadata": {
        "id": "0r5aLHHJ8uA1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datetimeFrame.head(5)"
      ],
      "metadata": {
        "id": "Luz17N2lCIry"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "timeseriesPlotterFrame = datetimeFrame.set_index(['datetime'])"
      ],
      "metadata": {
        "id": "hY7Mazb88uDt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "timeseriesPlotterFrame = timeseriesPlotterFrame.dropna()"
      ],
      "metadata": {
        "id": "tzDl_6Df1uBF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "timeseriesPlotterFrame"
      ],
      "metadata": {
        "id": "1DQbpSuSxp5j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "timeseriesPlotterFrame.plot()"
      ],
      "metadata": {
        "id": "T-ysG5yi8uGM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainTimeSeries = timeseriesPlotterFrame.copy().iloc[:round(0.8*timeseriesPlotterFrame.shape[0]),:] # Train set formed by first 80% of the original frame\n",
        "testTimeSeries = timeseriesPlotterFrame.copy().iloc[trainTimeSeries.shape[0]:,:] # Train set formed by the last 20% of the original frame\n",
        "valTimeSeries = trainTimeSeries.copy().iloc[round(0.8*trainTimeSeries.shape[0]):,:] # Val Set formed by the last 20% of the train frame\n",
        "trainTimeSeries = trainTimeSeries.copy().iloc[:round(0.8*trainTimeSeries.shape[0]),:] # Train frame left with first 80% of its previous version"
      ],
      "metadata": {
        "id": "9I3RVM1R8uNu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assert trainTimeSeries.shape[0] + testTimeSeries.shape[0] + valTimeSeries.shape[0] == timeseriesPlotterFrame.shape[0] # Verifying the size"
      ],
      "metadata": {
        "id": "W7MOZMTY8uTM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assert ( (pd.concat([trainTimeSeries, valTimeSeries, testTimeSeries]) == timeseriesPlotterFrame).sum() == timeseriesPlotterFrame.shape[0] )[0]# Verifying that split has happened properly by comparing the combined with the original frame"
      ],
      "metadata": {
        "id": "w1pZS1s7z8co"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader"
      ],
      "metadata": {
        "id": "DokU5ANN8uYS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Converting to torch tensors\n",
        "torch_train = torch.Tensor(trainTimeSeries.to_numpy())\n",
        "torch_test = torch.Tensor(testTimeSeries.to_numpy())\n",
        "torch_val = torch.Tensor(valTimeSeries.to_numpy())"
      ],
      "metadata": {
        "id": "QVbfyO7YwhlT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to get features\n",
        "def getFeaturesAndLabels(inp, window_size = 5):\n",
        "\n",
        "  featureTensor = torch.Tensor([])\n",
        "  labelTensor = torch.Tensor([])\n",
        "\n",
        "  for i in range(inp.shape[0] - window_size): # -1 because the last element will be a target\n",
        "    featureTensor = torch.cat([featureTensor, inp[i:i+window_size].t()]) \n",
        "\n",
        "  for i in range(window_size, inp.shape[0]):\n",
        "    labelTensor = torch.cat([labelTensor, inp[i].unsqueeze(0)])\n",
        "\n",
        "  return featureTensor, labelTensor"
      ],
      "metadata": {
        "id": "MAdHdKMrGxrY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "window_size = 4"
      ],
      "metadata": {
        "id": "XyD0lXa1lL_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_features, trainLabels = getFeaturesAndLabels(torch_train, window_size)\n",
        "train_features.shape, trainLabels.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ePaGVTwqJD_h",
        "outputId": "731d07be-4df3-4b6e-9e62-fce14230f52b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([21, 4]), torch.Size([21, 1]))"
            ]
          },
          "metadata": {},
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_features, valLabels = getFeaturesAndLabels(torch_val, window_size)\n",
        "val_features.shape, valLabels.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c6HTDT2hRD5F",
        "outputId": "2c46e23e-7e94-4d49-88e7-a5d535f92085"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([2, 4]), torch.Size([2, 1]))"
            ]
          },
          "metadata": {},
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_features, testLabels = getFeaturesAndLabels(torch_test, window_size)\n",
        "test_features.shape, testLabels.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DW5L402PREAZ",
        "outputId": "f8eddc34-6f29-4406-c369-ae5cc942a3b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([4, 4]), torch.Size([4, 1]))"
            ]
          },
          "metadata": {},
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train = TensorDataset(train_features, trainLabels)\n",
        "val = TensorDataset(val_features, valLabels)\n",
        "test = TensorDataset(test_features, testLabels)"
      ],
      "metadata": {
        "id": "_7ZIVTWIwhoE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 5"
      ],
      "metadata": {
        "id": "AbktERash9qX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(train, batch_size=batch_size, shuffle=False)\n",
        "val_loader = DataLoader(val, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test, batch_size=batch_size, shuffle=False)\n",
        "test_loader_one = DataLoader(test, batch_size=1, shuffle=False)"
      ],
      "metadata": {
        "id": "a9r5KIyPePXy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RNNModel(torch.nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim, dropout_prob):\n",
        "        super(RNNModel, self).__init__()\n",
        "\n",
        "        # Defining the number of layers and the nodes in each layer\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.layer_dim = layer_dim\n",
        "\n",
        "        # RNN layers\n",
        "        self.rnn = torch.nn.RNN(\n",
        "            input_dim, hidden_dim, layer_dim, batch_first=True, dropout=dropout_prob\n",
        "        )\n",
        "        # Fully connected layer\n",
        "        self.fc = torch.nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Initializing hidden state for first input with zeros\n",
        "        h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_()\n",
        "\n",
        "        # Forward propagation by passing in the input and hidden state into the model\n",
        "        out, h0 = self.rnn(x, h0.detach())\n",
        "\n",
        "        # Reshaping the outputs in the shape of (batch_size, seq_length, hidden_size)\n",
        "        # so that it can fit into the fully connected layer\n",
        "        out = out[:, -1, :]\n",
        "\n",
        "        # Convert the final state to our desired output shape (batch_size, output_dim)\n",
        "        out = self.fc(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "bt330w7mwhqw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model(model, model_params):\n",
        "    models = {\n",
        "        \"rnn\": RNNModel\n",
        "        # \"lstm\": LSTMModel,\n",
        "        # \"gru\": GRUModel,\n",
        "    }\n",
        "    return models.get(model.lower())(**model_params)"
      ],
      "metadata": {
        "id": "ea-JV1C4-u5e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Optimization:\n",
        "    def __init__(self, model, loss_fn, optimizer):\n",
        "        self.model = model\n",
        "        self.loss_fn = loss_fn\n",
        "        self.optimizer = optimizer\n",
        "        self.train_losses = []\n",
        "        self.val_losses = []\n",
        "    \n",
        "    def train_step(self, x, y):\n",
        "        # Sets model to train mode\n",
        "        self.model.train()\n",
        "\n",
        "        # Makes predictions\n",
        "        yhat = self.model(x)\n",
        "\n",
        "        # Computes loss\n",
        "        loss = self.loss_fn(y, yhat)\n",
        "\n",
        "        # Computes gradients\n",
        "        loss.backward()\n",
        "\n",
        "        # Updates parameters and zeroes gradients\n",
        "        self.optimizer.step()\n",
        "        self.optimizer.zero_grad()\n",
        "\n",
        "        # Returns the loss\n",
        "        return loss.item()\n",
        "\n",
        "    def train(self, train_loader, val_loader, batch_size=64, n_epochs=50, n_features=1):\n",
        "        # model_path = f'models/{self.model}_{datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}'\n",
        "\n",
        "        for epoch in range(1, n_epochs + 1):\n",
        "            batch_losses = []\n",
        "            for x_batch, y_batch in train_loader:\n",
        "                x_batch = x_batch.view([min(batch_size, x_batch.shape[0]), -1, n_features])\n",
        "                y_batch = y_batch\n",
        "                loss = self.train_step(x_batch, y_batch)\n",
        "                batch_losses.append(loss)\n",
        "            training_loss = np.mean(batch_losses)\n",
        "            self.train_losses.append(training_loss)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                batch_val_losses = []\n",
        "                for x_val, y_val in val_loader:\n",
        "                    x_val = x_val.view([min(batch_size, x_val.shape[0]), -1, n_features])\n",
        "                    y_val = y_val\n",
        "                    self.model.eval()\n",
        "                    yhat = self.model(x_val)\n",
        "                    val_loss = self.loss_fn(y_val, yhat).item()\n",
        "                    batch_val_losses.append(val_loss)\n",
        "                validation_loss = np.mean(batch_val_losses)\n",
        "                self.val_losses.append(validation_loss)\n",
        "\n",
        "            if (epoch <= 10) | (epoch % 50 == 0):\n",
        "                print(\n",
        "                    f\"[{epoch}/{n_epochs}] Training loss: {training_loss:.4f}\\t Validation loss: {validation_loss:.4f}\"\n",
        "                )\n",
        "\n",
        "\n",
        "    def evaluate(self, test_loader, batch_size=1, n_features=1):\n",
        "        with torch.no_grad():\n",
        "            predictions = []\n",
        "            values = []\n",
        "            for x_test, y_test in test_loader:\n",
        "                x_test = x_test.view([min(batch_size, x_test.shape[0]), -1, n_features])\n",
        "                y_test = y_test\n",
        "                self.model.eval()\n",
        "                yhat = self.model(x_test)\n",
        "                predictions.append(yhat.detach().numpy())\n",
        "                values.append(y_test.detach().numpy())\n",
        "\n",
        "        return predictions, values\n",
        "\n",
        "    def get_Model(self):\n",
        "     return self.model\n"
      ],
      "metadata": {
        "id": "xe_eT8a9BYX2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "\n",
        "input_dim = window_size\n",
        "print(input_dim)\n",
        "output_dim = 1\n",
        "hidden_dim = 64\n",
        "layer_dim = 3\n",
        "dropout = 0.2\n",
        "n_epochs = 300\n",
        "learning_rate = 1e-3\n",
        "weight_decay = 1e-6\n",
        "\n",
        "model_params = {'input_dim': input_dim,\n",
        "                'hidden_dim' : hidden_dim,\n",
        "                'layer_dim' : layer_dim,\n",
        "                'output_dim' : output_dim,\n",
        "                'dropout_prob' : dropout}\n",
        "\n",
        "model = get_model('rnn', model_params)\n",
        "\n",
        "loss_fn = torch.nn.MSELoss(reduction=\"mean\")\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "\n",
        "opt = Optimization(model=model, loss_fn=loss_fn, optimizer=optimizer)\n",
        "opt.train(train_loader, val_loader, batch_size=batch_size, n_epochs=n_epochs, n_features=input_dim)\n",
        "# opt.plot_losses()\n",
        "\n",
        "predictions, values = opt.evaluate(test_loader_one, batch_size=1, n_features=input_dim)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i0s5s2c56_pM",
        "outputId": "60560313-7fd2-4797-f9ea-85c58aac8e23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5\n",
            "[1/300] Training loss: 211.6105\t Validation loss: 336.1044\n",
            "[2/300] Training loss: 201.9517\t Validation loss: 313.9707\n",
            "[3/300] Training loss: 191.8343\t Validation loss: 291.7420\n",
            "[4/300] Training loss: 179.9974\t Validation loss: 270.7157\n",
            "[5/300] Training loss: 169.3514\t Validation loss: 251.0869\n",
            "[6/300] Training loss: 160.5145\t Validation loss: 233.1469\n",
            "[7/300] Training loss: 149.5741\t Validation loss: 217.2370\n",
            "[8/300] Training loss: 141.4016\t Validation loss: 203.3471\n",
            "[9/300] Training loss: 134.6356\t Validation loss: 191.3038\n",
            "[10/300] Training loss: 127.6016\t Validation loss: 180.6290\n",
            "[50/300] Training loss: 38.9992\t Validation loss: 28.3063\n",
            "[100/300] Training loss: 17.3936\t Validation loss: 1.4026\n",
            "[150/300] Training loss: 15.7879\t Validation loss: 0.0001\n",
            "[200/300] Training loss: 22.6197\t Validation loss: 0.0000\n",
            "[250/300] Training loss: 20.2719\t Validation loss: 0.0149\n",
            "[300/300] Training loss: 19.3204\t Validation loss: 0.0201\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trained_rnn_model = opt.get_Model()\n",
        "with torch.no_grad():\n",
        "  rnn_preds = trained_rnn_model(test_features.view([min(batch_size, test_features.shape[0]), -1, window_size]))\n"
      ],
      "metadata": {
        "id": "9IwXCkSUHgHX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rnn_preds.shape, testLabels.shape\n",
        "torch.nn.L1Loss()(rnn_preds.squeeze(1), testLabels.squeeze(1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PUtGE8h6ttUV",
        "outputId": "e5fe57f5-3265-43d8-d2a7-99829498d73c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(10.5038)"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_features.shape, train_features.shape, trainLabels.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "daZZg5fjHgKE",
        "outputId": "42edc32f-9715-4bff-d179-7ae93dc2e7c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([3, 5]), torch.Size([20, 5]), torch.Size([20, 1]))"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "D7k3_eSgHgM2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "kFOuGVpYHgPr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "DRxamyAEHgSm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Averaging ( Baseline )"
      ],
      "metadata": {
        "id": "C2k166CqryOR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mean Absolute Error\n",
        "torch.nn.L1Loss()(test_features.mean(axis = 1), testLabels.squeeze(1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0BThFa97_Z40",
        "outputId": "b688de4a-883a-4ed8-f6ea-69a7b2aa00c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(3.8195)"
            ]
          },
          "metadata": {},
          "execution_count": 256
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Linear Regression"
      ],
      "metadata": {
        "id": "rG1fIVNqtoHU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LinearRegressionModel(torch.nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "      super(LinearRegressionModel, self).__init__()\n",
        "      self.Linear = torch.nn.Sequential(\n",
        "          torch.nn.Linear(input_dim, 1)\n",
        "      )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.Linear(x)\n",
        "        return out"
      ],
      "metadata": {
        "id": "ji7cTkhMtnOp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(x_train, y_train, time_series_window_size = 5, learning_rate = 1e-3, weight_decay = 1e-6, bs = 64, epochs = 10):\n",
        "    model = LinearRegressionModel(time_series_window_size)\n",
        "\n",
        "    criterion = torch.nn.MSELoss(reduction=\"mean\")\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate, weight_decay = weight_decay)\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        print('epoch: {}'.format(epoch))\n",
        "        for i in range(0, max(1, int(round(x_train.shape[0]/bs)))):\n",
        "            tempX = x_train[i*bs:min(((i+1)*bs), x_train.shape[0])]\n",
        "            tempY = y_train[i*bs:min(((i+1)*bs), x_train.shape[0])]\n",
        "            tr_loss = 0\n",
        "            ptx_train, pty_train = torch.autograd.Variable(torch.Tensor(tempX)), torch.autograd.Variable(torch.Tensor(tempY))\n",
        "            optimizer.zero_grad()\n",
        "            output_train = model(ptx_train)\n",
        "            loss_train = criterion(output_train, pty_train)\n",
        "            loss_train.backward()\n",
        "            optimizer.step()\n",
        "            tr_loss = loss_train.item()\n",
        "    return model"
      ],
      "metadata": {
        "id": "fWqIMTlD_Z-I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "linreg_learning_rate = 1e-3\n",
        "linreg_weight_decay = 1e-6"
      ],
      "metadata": {
        "id": "j1sz897D1LEc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_features.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mJ4PqpAZ3JMW",
        "outputId": "6d04c9a0-e6b9-4765-8a03-856b30a95492"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([21, 4])"
            ]
          },
          "metadata": {},
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainedModel = train(train_features, trainLabels, window_size,  linreg_learning_rate, linreg_weight_decay)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T08gEzsV1g7V",
        "outputId": "116911ec-c9ef-4c9c-8fe0-843d9f949530"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 0\n",
            "epoch: 1\n",
            "epoch: 2\n",
            "epoch: 3\n",
            "epoch: 4\n",
            "epoch: 5\n",
            "epoch: 6\n",
            "epoch: 7\n",
            "epoch: 8\n",
            "epoch: 9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  linreg_preds = trainedModel(test_features)\n"
      ],
      "metadata": {
        "id": "OV86nz_N_aBG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.nn.L1Loss()(linreg_preds, testLabels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qc9pkOch_aD0",
        "outputId": "5159067e-4023-4d8b-e8c3-b5298bc4280f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(38.5045)"
            ]
          },
          "metadata": {},
          "execution_count": 111
        }
      ]
    }
  ]
}